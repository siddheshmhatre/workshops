{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, DatasetDict\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create instance of PAN-X dataset\n",
    "* Find config files for subsets of the above\n",
    "* Create a dataset with user defined proportions  \n",
    "* Add a column for human readable NER tags\n",
    "* Plot proportion of different tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"xtreme\"\n",
    "languages = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "proportions = [0.629, 0.229, 0.084, 0.059]\n",
    "configs = [config for config in get_dataset_config_names(dataset_name) if config.startswith(\"PAN-X\") and config.split('.')[1] in languages] # PAN-X.{2-letter ISO language code}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8646d98d56f8420da61217f329a07f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-50c6130fc2dbe6ad.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-3d878c38ca830baa.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c8fed2b7e6d59cbc.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15697e999adf40059a38241bfea99bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e14b50505509ca06.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-529925d4984531e4.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ef60137063549caf.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734586502a2943d9b9c5e64038b8c0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1d0cd9eb0adc8933.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e0dc2d749c429e9e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ca0a6f6c69069001.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb92e8da19f44e8a4e8f3f3b9d12c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-02142ffc6dacef09.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-79992a3f46d42fd5.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0e5a91e037304e5e.arrow\n"
     ]
    }
   ],
   "source": [
    "data_dict = defaultdict(DatasetDict)\n",
    "\n",
    "for config, lang, prop in zip(configs, languages, proportions):\n",
    "\tdata_dict[lang] = load_dataset(dataset_name, name=config)\n",
    "\tfor split, ds in data_dict[lang].items():\n",
    "\t\tdata_dict[lang][split] = ds.shuffle(seed).select(range(int(ds.num_rows * prop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = data_dict['de']['train'].features['ner_tags'].feature # ClassLabel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_names(x, mapping=mapping):\n",
    "\tx['ner_names'] = [mapping.int2str(idx) for idx in x['ner_tags']]\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in data_dict.keys():\n",
    "\tfor split in data_dict[lang].keys():\n",
    "\t\tdata_dict[lang][split] = data_dict[lang][split].map(create_ner_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = defaultdict(Counter)\n",
    "df_dict = defaultdict(list)\n",
    "\n",
    "for split in data_dict['de'].keys():\n",
    "\tdf_dict['split'].append(split)\n",
    "\tner_list = []\n",
    "\tfor lang in data_dict.keys():\n",
    "\t\tds = data_dict[lang][split]\n",
    "\n",
    "\t\tfor item in ds:\n",
    "\t\t\tner_list.extend([tag for tag in item['ner_names'] if tag.startswith('B-')])\n",
    "\tcounts_dict[split] = Counter(ner_list)\n",
    "\n",
    "\tfor key, value in counts_dict[split].items():\n",
    "\t\tdf_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>8686</td>\n",
       "      <td>9241</td>\n",
       "      <td>9725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation</td>\n",
       "      <td>4333</td>\n",
       "      <td>4623</td>\n",
       "      <td>4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>4317</td>\n",
       "      <td>4756</td>\n",
       "      <td>4893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split  B-ORG  B-PER  B-LOC\n",
       "0       train   8686   9241   9725\n",
       "1  validation   4333   4623   4875\n",
       "2        test   4317   4756   4893"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {'split' : [], 'B-ORG' : [], 'B-PER' : [], 'B-LOC' : []}\n",
    "\n",
    "for split in counts_dict:\n",
    "\tdf_dict['split'].append(split)\n",
    "\n",
    "\tfor key in counts_dict[split].keys():\n",
    "\t\tdf_dict[key].append(counts_dict[split][key])\n",
    "\n",
    "pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Lingual Transformers and Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mBERT is eclipsed by XLM-RoBERTa. Differences are the following -\n",
    "\t* Trained on orders of magnitude larger dataset\n",
    "\t* NSP not used in pre-training objective\n",
    "\t* SentencePiece used as tokenizer instead of WordPiece\n",
    "* Explore using SP and WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "\n",
    "word_piece = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence_piece = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece : ['[CLS]', 'jack', 'sparrow', 'loves', 'new', 'york', '[SEP]']\n",
      "SentencePiece : ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Jack Sparrow loves New York\"\n",
    "print (f\"WordPiece : {word_piece(sentence).tokens()}\")\n",
    "print (f\"SentencePiece : {sentence_piece(sentence).tokens()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York</s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenized = sentence_piece(sentence).tokens()\n",
    "\"\".join([tok.replace(u\"\\u2581\", \" \") for tok in sp_tokenized])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual NER model, Tokenize datasets, Performance metrics\n",
    "* Implement XLM-R for token classification using pretrained XLM-R\n",
    "* Write a helper function that tokenizes -> model.forward\n",
    "* Function to tokenize entire dataset and take into consideration attention masks\n",
    "* Play around with seqeval for evaluating token classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "\tconfig_class = XLMRobertaConfig\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__(config)\n",
    "\n",
    "\t\tself.num_labels = config.num_labels\n",
    "\n",
    "\t\t# Rename to roberta \n",
    "\t\tself.roberta = RobertaModel(config, add_pooling_layer=True)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\tself.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "\t\tself.init_weights()\n",
    "\n",
    "\tdef forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\t\toutputs = self.roberta(input_ids, attention_mask, token_type_ids, **kwargs)\n",
    "\n",
    "\t\tsequence_op = self.dropout(outputs[0])\n",
    "\t\tlogits = self.classifier(sequence_op)\n",
    "\n",
    "\t\tloss = None\n",
    "\n",
    "\t\tif labels is not None:\n",
    "\t\t\tloss = F.cross_entropy(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "\t\treturn TokenClassifierOutput(loss, logits, outputs.hidden_states, outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/siddhesh1793/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(mapping.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(mapping.names)}\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=mapping.num_classes, id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home/siddhesh1793/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ids</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4  5     6      7     8\n",
       "Tokens  <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York  </s>\n",
       "Ids       0  21763  37456  15555   5161  7  2356   5753     2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = sentence_piece.encode(sentence, return_tensors=\"pt\")\n",
    "xlmr_tokens = sentence_piece(sentence).tokens()\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=['Tokens', 'Ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xlmr_model(input_ids.to(device))\n",
    "preds = torch.argmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preds</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York   </s>\n",
       "Preds   I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_str = [mapping.names[idx] for idx in preds[0]]\n",
    "pd.DataFrame([xlmr_tokens, preds_str], index=['Tokens', 'Preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Jack Sparrow loves New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Sparrow loves New York\n",
      "[('▁Jack', (0, 4)), ('▁Sparrow', (5, 12)), ('▁loves', (13, 18)), ('▁New', (19, 22)), ('▁York', (23, 27))]\n",
      "[<tokenizers.Token object at 0x7fbc59ca7da0>, <tokenizers.Token object at 0x7fbc59cae080>, <tokenizers.Token object at 0x7fbc59cae670>, <tokenizers.Token object at 0x7fbc59cae6c0>, <tokenizers.Token object at 0x7fbc59cae710>, <tokenizers.Token object at 0x7fbc59cae760>, <tokenizers.Token object at 0x7fbc59cae7b0>, <tokenizers.Token object at 0x7fbc59cae800>, <tokenizers.Token object at 0x7fbc59cae850>, <tokenizers.Token object at 0x7fbc59cae8a0>, <tokenizers.Token object at 0x7fbc59cae8f0>]\n"
     ]
    }
   ],
   "source": [
    "sp_backend_tokenizer = sentence_piece.backend_tokenizer\t\n",
    "normalized_str = sp_backend_tokenizer.normalizer.normalize_str(sentence)\n",
    "print(normalized_str)\n",
    "pretokenized_str = sp_backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized_str)\n",
    "print(pretokenized_str)\n",
    "tokenized_str = sp_backend_tokenizer.model.tokenize(normalized_str)\n",
    "print(tokenized_str)\n",
    "#\n",
    "#sentence_piece(sentence).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack sparrow loves new york\n",
      "[('jack', (0, 4)), ('sparrow', (5, 12)), ('loves', (13, 18)), ('new', (19, 22)), ('york', (23, 27))]\n",
      "[<tokenizers.Token object at 0x7fbc59c14ad0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'jack', 'sparrow', 'loves', 'new', 'york', '[SEP]']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_backend_tokenizer = word_piece.backend_tokenizer\t\n",
    "normalized_str = sp_backend_tokenizer.normalizer.normalize_str(sentence)\n",
    "print(normalized_str)\n",
    "pretokenized_str = sp_backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized_str)\n",
    "print(pretokenized_str)\n",
    "tokenized_str = sp_backend_tokenizer.model.tokenize(normalized_str)\n",
    "print(tokenized_str)\n",
    "\n",
    "word_piece(sentence).tokens()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50ace8a3ab34cfbcb3acf31def95bf386d39a973007f8dddaa4307b74d7c07a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
