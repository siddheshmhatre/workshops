{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, DatasetDict\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create instance of PAN-X dataset\n",
    "* Find config files for subsets of the above\n",
    "* Create a dataset with user defined proportions  \n",
    "* Add a column for human readable NER tags\n",
    "* Plot proportion of different tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"xtreme\"\n",
    "languages = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "proportions = [0.629, 0.229, 0.084, 0.059]\n",
    "configs = [config for config in get_dataset_config_names(dataset_name) if config.startswith(\"PAN-X\") and config.split('.')[1] in languages] # PAN-X.{2-letter ISO language code}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337f1d56b675469eaf14efa778dcf760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-50c6130fc2dbe6ad.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-3d878c38ca830baa.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c8fed2b7e6d59cbc.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340221e3937e4a7da855e8252d5fec22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e14b50505509ca06.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-529925d4984531e4.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ef60137063549caf.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726ffa47333547bc937dbec1efd50897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1d0cd9eb0adc8933.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e0dc2d749c429e9e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ca0a6f6c69069001.arrow\n",
      "Found cached dataset xtreme (/home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a0cff6c9c4877935a58c47f3e2560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-02142ffc6dacef09.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-79992a3f46d42fd5.arrow\n",
      "Loading cached shuffled indices for dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0e5a91e037304e5e.arrow\n"
     ]
    }
   ],
   "source": [
    "data_dict = defaultdict(DatasetDict)\n",
    "\n",
    "for config, lang, prop in zip(configs, languages, proportions):\n",
    "\tdata_dict[lang] = load_dataset(dataset_name, name=config)\n",
    "\tfor split, ds in data_dict[lang].items():\n",
    "\t\tdata_dict[lang][split] = ds.shuffle(seed).select(range(int(ds.num_rows * prop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = data_dict['de']['train'].features['ner_tags'].feature # ClassLabel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_names(x, mapping=mapping):\n",
    "\tx['ner_names'] = [mapping.int2str(idx) for idx in x['ner_tags']]\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a4714dc4be797803.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-86d4a1d8fd82ddbd.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-c6112e841ec8c9b1.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0ef085b80e9e8e08.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-bc3b8df0625b3ce3.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-60dfb63e24a2aac4.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-42285d6579cb0011.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-184495b071a1052c.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0700b686a5c34fa1.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-48b25c3ce3b0fa53.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a854217f179c6098.arrow\n",
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0d3444aa1c9de2f5.arrow\n"
     ]
    }
   ],
   "source": [
    "for lang in data_dict.keys():\n",
    "\tfor split in data_dict[lang].keys():\n",
    "\t\tdata_dict[lang][split] = data_dict[lang][split].map(create_ner_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict = defaultdict(Counter)\n",
    "df_dict = defaultdict(list)\n",
    "\n",
    "for split in data_dict['de'].keys():\n",
    "\tdf_dict['split'].append(split)\n",
    "\tner_list = []\n",
    "\tfor lang in data_dict.keys():\n",
    "\t\tds = data_dict[lang][split]\n",
    "\n",
    "\t\tfor item in ds:\n",
    "\t\t\tner_list.extend([tag for tag in item['ner_names'] if tag.startswith('B-')])\n",
    "\tcounts_dict[split] = Counter(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>8686</td>\n",
       "      <td>9241</td>\n",
       "      <td>9725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation</td>\n",
       "      <td>4333</td>\n",
       "      <td>4623</td>\n",
       "      <td>4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>4317</td>\n",
       "      <td>4756</td>\n",
       "      <td>4893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split  B-ORG  B-PER  B-LOC\n",
       "0       train   8686   9241   9725\n",
       "1  validation   4333   4623   4875\n",
       "2        test   4317   4756   4893"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {'split' : [], 'B-ORG' : [], 'B-PER' : [], 'B-LOC' : []}\n",
    "\n",
    "for split in counts_dict:\n",
    "\tdf_dict['split'].append(split)\n",
    "\n",
    "\tfor key in counts_dict[split].keys():\n",
    "\t\tdf_dict[key].append(counts_dict[split][key])\n",
    "\n",
    "pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Lingual Transformers and Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mBERT is eclipsed by XLM-RoBERTa. Differences are the following -\n",
    "\t* Trained on orders of magnitude larger dataset\n",
    "\t* NSP not used in pre-training objective\n",
    "\t* SentencePiece used as tokenizer instead of WordPiece\n",
    "* Explore using SP and WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "\n",
    "word_piece = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence_piece = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece : ['[CLS]', 'jack', 'sparrow', 'loves', 'new', 'york', '[SEP]']\n",
      "SentencePiece : ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Jack Sparrow loves New York\"\n",
    "print (f\"WordPiece : {word_piece(sentence).tokens()}\")\n",
    "print (f\"SentencePiece : {sentence_piece(sentence).tokens()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenized = sentence_piece(sentence).tokens()\n",
    "\"\".join([tok.replace(u\"\\u2581\", \" \") for tok in sp_tokenized])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual NER model, Tokenize datasets, Performance metrics\n",
    "* Implement XLM-R for token classification using pretrained XLM-R\n",
    "* Write a helper function that tokenizes -> model.forward\n",
    "* Function to tokenize entire dataset and take into consideration attention masks\n",
    "* Play around with seqeval for evaluating token classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "\tconfig_class = XLMRobertaConfig\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__(config)\n",
    "\n",
    "\t\tself.num_labels = config.num_labels\n",
    "\n",
    "\t\t# Rename to roberta \n",
    "\t\tself.roberta = RobertaModel(config, add_pooling_layer=True)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\tself.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "\t\tself.init_weights()\n",
    "\n",
    "\tdef forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "\t\toutputs = self.roberta(input_ids, attention_mask, token_type_ids, **kwargs)\n",
    "\n",
    "\t\tsequence_op = self.dropout(outputs[0])\n",
    "\t\tlogits = self.classifier(sequence_op)\n",
    "\n",
    "\t\tloss = None\n",
    "\n",
    "\t\tif labels is not None:\n",
    "\t\t\tloss = F.cross_entropy(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "\t\treturn TokenClassifierOutput(loss, logits, outputs.hidden_states, outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(mapping.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(mapping.names)}\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=mapping.num_classes, id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ids</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4  5     6      7     8\n",
       "Tokens  <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York  </s>\n",
       "Ids       0  21763  37456  15555   5161  7  2356   5753     2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = sentence_piece(sentence, return_tensors='pt')\n",
    "xlmr_tokens = op.tokens()\n",
    "input_ids = op.input_ids\n",
    "pd.DataFrame([xlmr_tokens, input_ids.numpy().tolist()[0]], index=['Tokens', 'Ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xlmr_model(input_ids.to(device))\n",
    "preds = torch.argmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preds</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York   </s>\n",
       "Preds   I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_str = [mapping.names[idx] for idx in preds[0]]\n",
    "pd.DataFrame([xlmr_tokens, preds_str], index=['Tokens', 'Preds'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - wrap above in one function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Texts\n",
    "* We want to tokenize all the samples in the dataset and also assign labels to each word in each sample\n",
    "* Need to define a function with the signature - func(x : Dict[str, List]) -> Dict[str, List]\n",
    "* Need to keep in mind the convention that only the first token of a word is assigned the NER tag, subsequent tokens are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data_dict['de']['train'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sentence_piece(example['tokens'], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Olymp</td>\n",
       "      <td>ique</td>\n",
       "      <td>▁N</td>\n",
       "      <td>îm</td>\n",
       "      <td>es</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Au</td>\n",
       "      <td>xer</td>\n",
       "      <td>...</td>\n",
       "      <td>▁die</td>\n",
       "      <td>▁Haupt</td>\n",
       "      <td>runde</td>\n",
       "      <td>▁quali</td>\n",
       "      <td>fi</td>\n",
       "      <td>zieren</td>\n",
       "      <td>▁können</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word Ids</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1     2   3   4   5  6  7    8    9   ...    28      29  \\\n",
       "Tokens     <s>  ▁Olymp  ique  ▁N  îm  es  ▁  ,  ▁Au  xer  ...  ▁die  ▁Haupt   \n",
       "Word Ids  None       0     0   1   1   1  2  2    3    3  ...    14      15   \n",
       "\n",
       "             30      31  32      33       34  35  36    37  \n",
       "Tokens    runde  ▁quali  fi  zieren  ▁können   ▁   .  </s>  \n",
       "Word Ids     15      16  16      16       17  18  18  None  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokenized.tokens(), tokenized.word_ids()], index=['Tokens', 'Word Ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "word_ids = tokenized.word_ids()\n",
    "\n",
    "for idx, word_idx in enumerate(word_ids):\n",
    "\tif word_idx is None:\n",
    "\t\tlabel_ids.append(-100)\n",
    "\telif word_idx != previous_word_idx:\n",
    "\t\tlabel_ids.append(example['ner_tags'][word_idx])\n",
    "\telse:\n",
    "\t\tlabel_ids.append(-100)\n",
    "\n",
    "\tprevious_word_idx = word_idx\n",
    "labels = [mapping.names[idx] if idx != -100 else None for idx in label_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Olymp</td>\n",
       "      <td>ique</td>\n",
       "      <td>▁N</td>\n",
       "      <td>îm</td>\n",
       "      <td>es</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Au</td>\n",
       "      <td>xer</td>\n",
       "      <td>...</td>\n",
       "      <td>▁die</td>\n",
       "      <td>▁Haupt</td>\n",
       "      <td>runde</td>\n",
       "      <td>▁quali</td>\n",
       "      <td>fi</td>\n",
       "      <td>zieren</td>\n",
       "      <td>▁können</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>None</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label Ids</th>\n",
       "      <td>-100</td>\n",
       "      <td>3</td>\n",
       "      <td>-100</td>\n",
       "      <td>4</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word Ids</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1     2      3     4     5  6     7    8     9   ...  \\\n",
       "Tokens      <s>  ▁Olymp  ique     ▁N    îm    es  ▁     ,  ▁Au   xer  ...   \n",
       "Labels     None   B-ORG  None  I-ORG  None  None  O  None    O  None  ...   \n",
       "Label Ids  -100       3  -100      4  -100  -100  0  -100    0  -100  ...   \n",
       "Word Ids   None       0     0      1     1     1  2     2    3     3  ...   \n",
       "\n",
       "             28      29     30      31    32      33       34  35    36    37  \n",
       "Tokens     ▁die  ▁Haupt  runde  ▁quali    fi  zieren  ▁können   ▁     .  </s>  \n",
       "Labels        O       O   None       O  None    None        O   O  None  None  \n",
       "Label Ids     0       0   -100       0  -100    -100        0   0  -100  -100  \n",
       "Word Ids     14      15     15      16    16      16       17  18    18  None  \n",
       "\n",
       "[4 rows x 38 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokenized.tokens(), labels, label_ids, word_ids], index=['Tokens', 'Labels', 'Label Ids', 'Word Ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(examples):\n",
    "\ttokenized = sentence_piece(examples['tokens'], is_split_into_words=True, truncation=True)\n",
    "\n",
    "\tlabels = []\n",
    "\tsubword_tokens = []\n",
    "\n",
    "\tfor idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "\t\tlabel_ids = []\n",
    "\t\tprevious_word_idx = None\n",
    "\n",
    "\t\tfor word_id in tokenized.word_ids(batch_index=idx):\n",
    "\t\t\tif word_id is None:\n",
    "\t\t\t\tlabel_ids.append(-100)\n",
    "\t\t\telif word_id != previous_word_idx:\n",
    "\t\t\t\tlabel_ids.append(label[word_id])\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabel_ids.append(-100)\n",
    "\n",
    "\t\t\tprevious_word_idx = word_id\n",
    "\t\tlabels.append(label_ids)\n",
    "\t\tsubword_tokens.append(tokenized.tokens(batch_index=idx))\n",
    "\n",
    "\ttokenized['labels'] = labels\n",
    "\ttokenized['subword_tokens'] = subword_tokens\n",
    "\treturn tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa803784005944d78a0968edc9529772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_ds = data_dict['de']['train'].map(encode_dataset, batched=True, remove_columns=['tokens', 'ner_tags', 'langs', 'ner_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Olymp</td>\n",
       "      <td>ique</td>\n",
       "      <td>▁N</td>\n",
       "      <td>îm</td>\n",
       "      <td>es</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁Au</td>\n",
       "      <td>xer</td>\n",
       "      <td>...</td>\n",
       "      <td>▁die</td>\n",
       "      <td>▁Haupt</td>\n",
       "      <td>runde</td>\n",
       "      <td>▁quali</td>\n",
       "      <td>fi</td>\n",
       "      <td>zieren</td>\n",
       "      <td>▁können</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token Ids</th>\n",
       "      <td>0</td>\n",
       "      <td>68237</td>\n",
       "      <td>11503</td>\n",
       "      <td>541</td>\n",
       "      <td>52644</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5655</td>\n",
       "      <td>34058</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>47582</td>\n",
       "      <td>95611</td>\n",
       "      <td>14768</td>\n",
       "      <td>1029</td>\n",
       "      <td>117484</td>\n",
       "      <td>2556</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label Ids</th>\n",
       "      <td>-100</td>\n",
       "      <td>3</td>\n",
       "      <td>-100</td>\n",
       "      <td>4</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1      2    3      4     5  6     7     8      9   ...  \\\n",
       "Tokens      <s>  ▁Olymp   ique   ▁N     îm    es  ▁     ,   ▁Au    xer  ...   \n",
       "Token Ids     0   68237  11503  541  52644    90  6     4  5655  34058  ...   \n",
       "Label Ids  -100       3   -100    4   -100  -100  0  -100     0   -100  ...   \n",
       "\n",
       "             28      29     30      31    32      33       34 35    36    37  \n",
       "Tokens     ▁die  ▁Haupt  runde  ▁quali    fi  zieren  ▁können  ▁     .  </s>  \n",
       "Token Ids    68   47582  95611   14768  1029  117484     2556  6     5     2  \n",
       "Label Ids     0       0   -100       0  -100    -100        0  0  -100  -100  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([new_ds[0]['subword_tokens'], new_ds[0]['input_ids'], new_ds[0]['labels']], index=['Tokens', 'Token Ids', 'Label Ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "* We use Precision, Recall and F1-score\n",
    "* For an entity to be classified correctly all the words corresponding to that entity should be correctly classified\n",
    "* seqeval package is used to compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "labels = [['O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']]\n",
    "preds = [['O', 'O', 'I-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds))#, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NP       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         1\n",
      "   macro avg       0.00      0.00      0.00         1\n",
      "weighted avg       0.00      0.00      0.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [['B-NP', 'I-NP', 'O' ]]\n",
    "preds = [['I-NP', 'I-NP', 'O' ]]\n",
    "print(classification_report(y_true, preds, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - Understand strict vs not strict better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50ace8a3ab34cfbcb3acf31def95bf386d39a973007f8dddaa4307b74d7c07a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
